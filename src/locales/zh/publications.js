export default {
  'title': '发表成果',
  'buttons': {
    'project': '项目',
    'arxiv': '预印本',
    'pdf': 'PDF文档',
    'code': '代码',
    'click': '点击查看详情',
    'abstract': '摘要',
    'contributions': '主要贡献'
  },
  'data': [
    {
      title: 'OpenVLA：一个开源的视觉-语言-动作模型',
      date: '2024年6月',
      image: new URL('../../assets/publications/openvla.png', import.meta.url).href,
      description: '一个<span class="highlight-red">70亿参数</span>的开源视觉-语言-动作模型，基于<span class="highlight-red">97万</span>个真实世界机器人演示样本的多样化集合进行训练。',
      tags: ['机器人学', '视觉-语言-动作模型'],
      links: {
        arxiv: 'http://arxiv.org/abs/2406.09246',
        pdf: 'http://arxiv.org/pdf/2406.09246',
        code: 'https://github.com/openvla/openvla'
      },
      abstract: '在互联网规模的视觉-语言数据和多样化机器人演示样本相结合的基础上预训练的大型策略，有望改变我们教授机器人新技能的方式：我们无需从头开始训练新行为，而是可以通过微调此类视觉-语言-动作（VLA）模型，获得用于视觉运动控制的稳健且可泛化的策略。然而，VLA在机器人领域的广泛应用面临着挑战：1）现有VLA模型大多是封闭的，公众无法获取；2）先前的研究未能探索有效微调VLA以适应新任务的方法，而这是实现广泛应用的关键环节。为解决这些挑战，我们提出了OpenVLA，这是一个70亿参数的开源VLA模型，基于97万个真实世界机器人演示样本的多样化集合训练而成。OpenVLA以Llama 2语言模型为基础，结合了一个视觉编码器，该编码器融合了来自DINOv2和SigLIP的预训练特征。由于增加的数据多样性和新的模型组件，OpenVLA在通用操作方面表现出色，在29项任务和多种机器人形态中，其绝对任务成功率比RT-2-X（550亿参数）等封闭模型高出16.5%，且参数数量少7倍。我们进一步表明，我们可以有效地微调OpenVLA以适应新场景，在涉及多个对象的多任务环境中展现出强大的泛化能力和语言接地能力，并且比Diffusion Policy等从头开始的模仿学习方法高出20.4%。我们还探索了计算效率；作为另一项贡献，我们表明OpenVLA可以通过现代低秩适应方法在消费级GPU上进行微调，并通过量化高效部署，且不会影响下游任务的成功率。最后，我们发布了模型检查点、微调笔记本以及我们的PyTorch代码库，该代码库内置支持在Open X-Embodiment数据集上大规模训练VLA。',
      contributions: [
        '我们提出了OpenVLA，这是一个70亿参数的开源视觉-语言-动作模型，基于97万个真实世界机器人演示样本的多样化集合训练而成，在29项任务和多种机器人形态中，其绝对任务成功率比RT-2-X（550亿参数）等封闭模型高出16.5%。',
        '我们发现了现有VLA条件提示调优方法中的关键问题，并提出了有效的解决方案，从而开发出一种新的最先进的条件提示调优方法，比先前的方法高出3.49%。',
        '我们证明了OpenVLA可以有效地微调以适应新场景，在多任务环境中取得了强大的泛化结果，并且比Diffusion Policy等从头开始的模仿学习方法高出20.4%。',
        '我们探索了计算效率技术，表明OpenVLA可以通过低秩适应方法在消费级GPU上进行微调，并通过量化高效部署，且不会影响下游任务的成功率。'
      ]
    },
    {
      title: '<span class="highlight-red">ICML 2021</span> 从自然语言监督中学习可迁移的视觉模型',
      date: '2023年11月',
      image: new URL('../../assets/publications/clip.png', import.meta.url).href,
      description: '一种通过预测哪个标题与哪个图像匹配，从自然语言监督中学习可迁移视觉模型的方法，使用从互联网收集的<span class="highlight-red">4亿</span>个（图像、文本）对数据集。',
      tags: ['视觉-语言模型', '对比学习'],
      links: {
        arxiv: 'http://arxiv.org/abs/2103.00020',
        pdf: 'http://arxiv.org/abs/2103.00020',
        code: 'https://github.com/openai/CLIP'
      },
      abstract: '最先进的计算机视觉系统是为预测一组固定的预定对象类别而训练的。这种受限形式的监督限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从关于图像的原始文本中学习是一种很有前景的替代方案，它利用了更广泛的监督来源。我们证明，预测哪个标题与哪个图像匹配这一简单的预训练任务，是在从互联网收集的4亿个（图像、文本）对数据集上从头开始学习最先进图像表示的一种高效且可扩展的方法。预训练后，自然语言被用于引用已学习的视觉概念（或描述新的概念），使模型能够零样本迁移到下游任务。我们通过在30多个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，涵盖的任务包括光学字符识别（OCR）、视频中的动作识别、地理定位以及多种类型的细粒度对象分类。该模型能够非平凡地迁移到大多数任务，并且通常与完全监督的基线相当，而无需任何特定于数据集的训练。例如，我们在ImageNet零样本任务上匹配了原始ResNet-50的准确率，而无需使用它所训练的128万个训练样本中的任何一个。我们在https://github.com/OpenAI/CLIP上发布了我们的代码和预训练模型权重。',
      contributions: [
        '我们提出了一种通过预测哪个标题与哪个图像匹配，从自然语言监督中学习可迁移视觉模型的方法，使用从互联网收集的4亿个（图像、文本）对数据集。',
        '我们证明，我们的方法能够使模型零样本迁移到30多个不同的现有计算机视觉数据集，在无需任何特定于数据集的训练的情况下，取得与完全监督基线相当的性能。',
        '我们表明，我们的模型可以在零样本设置下匹配原始ResNet-50在ImageNet上的准确率，而无需使用它所训练的128万个训练样本中的任何一个。'
      ]
    }
  ]
}